title,body,labels,milestone
"Install & Configure ROS 2 Humble","Goal: Install ROS 2 Humble on Ubuntu 22.04 and verify basic functionality.\n\nTasks:\n- Install ROS 2 Humble from official repositories.\n- Install desktop-full (includes Gazebo).\n- Source setup.bash correctly.\n- Run sample talker/listener demo.\n\nAcceptance Criteria:\n- `ros2 --version` shows Humble.\n- Gazebo is installed and starts.\n- Demo talker/listener nodes communicate without errors.","setup;ros2;infrastructure;critical","Milestone 1"
"Set Up ROS 2 Workspace & Packages","Goal: Create ROS 2 workspace and base package layout for the project.\n\nTasks:\n- Create `~/ros2_ws` and `src/` structure.\n- Create `drone_control` package (Python).\n- Create `gazebo_sim` package for launch/world files.\n- Add `package.xml` and `setup.py` where needed.\n- Build with colcon and fix any issues.\n\nAcceptance Criteria:\n- `colcon build` succeeds.\n- `source install/setup.bash` works.\n- Packages are visible via `ros2 pkg list`.","setup;ros2;infrastructure;high","Milestone 1"
"Install & Configure Gazebo with Drone Model","Goal: Have a drone model (e.g., iris) spawning correctly in Gazebo.\n\nTasks:\n- Install/configure Gazebo (if not done via desktop-full).\n- Add or clone a quadrotor/drone model (e.g., ArduPilot iris).\n- Create a simple world file with ground plane and a few objects.\n- Create a ROS 2 launch file to start Gazebo and spawn the drone.\n\nAcceptance Criteria:\n- `ros2 launch ...` starts Gazebo with the drone.\n- Drone appears with physics enabled.\n- World loads without errors.","gazebo;simulation;drone-model;critical","Milestone 1"
"Add Camera Plugin to Drone","Goal: Attach an RGB camera to the drone in Gazebo and expose it via ROS 2.\n\nTasks:\n- Modify the drone URDF/SDF to add an RGB camera sensor.\n- Configure resolution (e.g., 640x480) and frame rate (~30 FPS).\n- Add `gazebo_ros_camera` plugin.\n- Remap image topic to `/drone/camera/rgb`.\n- Verify topic exists and publishes images.\n\nAcceptance Criteria:\n- `/drone/camera/rgb` appears in `ros2 topic list`.\n- `ros2 run image_view image_view` shows the camera feed.\n- Frame rate is stable and images look correct.","gazebo;sensor;camera;critical","Milestone 1"
"Implement Basic ROS 2 Drone Controller Node","Goal: Create a Python ROS 2 node to interface with the drone and camera.\n\nTasks:\n- Create `drone_controller.py` using `rclpy`.\n- Subscribe to `/drone/camera/rgb` and store latest frame.\n- Subscribe to `/drone/state` or `/drone/odom`.\n- Publish `geometry_msgs/Twist` on `/cmd_vel` for movement.\n- Add basic logging of received frames and state.\n\nAcceptance Criteria:\n- Node starts without errors.\n- Drone responds to `/cmd_vel` published by the node.\n- Camera frames are received and logged.","ros2;python;drone-control;high","Milestone 1"
"Test Manual Drone Movement via ROS 2","Goal: Validate that basic movement commands correctly move the drone in Gazebo.\n\nTasks:\n- Use `ros2 topic pub` to send Twist commands to `/cmd_vel`.\n- Test forward, backward, left, right, up, down, yaw in both directions.\n- Confirm movement in the Gazebo UI.\n- Confirm state topics update accordingly.\n\nAcceptance Criteria:\n- All basic motions work and are visible in the simulator.\n- No unexpected drift or instability beyond physics settings.\n- No errors in ROS 2 logs during commands.","testing;ros2;drone-control;high","Milestone 1"
"Verify Camera Feed & Image Quality","Goal: Ensure the camera stream is usable for later computer vision.\n\nTasks:\n- View live camera feed via `image_view` or custom tool.\n- Measure topic frequency with `ros2 topic hz /drone/camera/rgb`.\n- Capture sample frames to disk for later testing.\n- Check resolution, color correctness, and noise.\n\nAcceptance Criteria:\n- Stable frame rate (≈30 FPS or configured value).\n- Images are clear and correctly oriented.\n- Resolution and encoding match expectations (e.g., RGB8, 640x480).","testing;camera;sensor;medium","Milestone 1"
"Set Up FastAPI Project Structure","Goal: Initialize FastAPI web server for controlling the drone.\n\nTasks:\n- Create `web_server/` folder and Python virtualenv.\n- Add `main.py` with a minimal FastAPI app.\n- Create `routes/` package for organization.\n- Add `requirements.txt` (fastapi, uvicorn, opencv-python, etc.).\n- Verify server runs with `uvicorn main:app --reload`.\n\nAcceptance Criteria:\n- Hitting `/health` or root returns a simple JSON/HTML response.\n- Dependencies install cleanly in the virtual environment.","setup;fastapi;web-server;critical","Milestone 2"
"Implement ROS 2 ↔ FastAPI Bridge","Goal: Connect FastAPI routes to ROS 2 via a threaded bridge.\n\nTasks:\n- Create `ros_bridge.py` with a `ROSBridge` class.\n- Initialize `rclpy` and a node inside the bridge.\n- Run ROS 2 executor in a separate daemon thread.\n- Implement methods: `publish_cmd_vel`, `get_latest_image`, `get_drone_state`.\n- Use a small, thread-safe buffer (e.g., deque) for images.\n\nAcceptance Criteria:\n- FastAPI stays responsive while ROS 2 spins.\n- Bridge can publish and subscribe without deadlocks.\n- No crashes when starting/stopping the server.","ros2;fastapi;threading;critical","Milestone 2"
"Implement Manual Control API Routes","Goal: Provide HTTP endpoints for basic drone movements and status.\n\nTasks:\n- Add `POST /control/move` accepting direction and speed.\n- Map directions (forward, back, left, right, up, down, rotate) to Twist.\n- Add `POST /control/stop` to zero velocities.\n- Add `GET /drone/status` returning position and basic state.\n- Validate requests with Pydantic models.\n\nAcceptance Criteria:\n- Calling `/control/move` triggers visible movement.\n- `/control/stop` reliably stops the drone.\n- `/drone/status` returns sensible state data.","fastapi;routes;control;high","Milestone 2"
"Implement Camera Stream Endpoint","Goal: Stream the drone camera to the browser via HTTP MJPEG.\n\nTasks:\n- Add `GET /camera/stream` endpoint.\n- Convert latest ROS image to OpenCV, then JPEG bytes.\n- Implement MJPEG multipart response with a frame loop.\n- Tune resolution/frame rate for low latency.\n\nAcceptance Criteria:\n- `<img src=\"/camera/stream\">` shows a live video.\n- Latency is acceptable for manual control.\n- Stream runs for 30+ minutes without crashing.","fastapi;camera;streaming;high","Milestone 2"
"Build Basic Web UI for Manual Control","Goal: Create a simple browser UI for controlling the drone and viewing the camera.\n\nTasks:\n- Create `web_ui/index.html`, `style.css`, `script.js`.\n- Display the camera stream in an `<img>` tag.\n- Add buttons for movement directions and stop.\n- Add a speed slider and show basic status text.\n- Implement keyboard shortcuts for movement.\n\nAcceptance Criteria:\n- UI loads from a static route or dev server.\n- Buttons and keys correctly call the API.\n- Camera feed is visible in the page.","frontend;html;javascript;high","Milestone 2"
"End-to-End Test: Manual Control via Web","Goal: Verify full path UI → FastAPI → ROS → Gazebo → Camera.\n\nTasks:\n- Use the web UI to move the drone in all directions.\n- Confirm movement in Gazebo.\n- Confirm camera perspective changes.\n- Try rapid button presses and different speeds.\n\nAcceptance Criteria:\n- All motions work from the browser.\n- No critical errors in FastAPI or ROS logs.\n- UI remains responsive during control and streaming.","testing;integration;manual-control;high","Milestone 2"
"Add Logging & Error Handling","Goal: Improve observability and robustness of the web server.\n\nTasks:\n- Configure Python logging to file and console.\n- Add logs to key routes and ROS bridge operations.\n- Add try/except around ROS calls and image handling.\n- Return clear JSON error messages on failures.\n\nAcceptance Criteria:\n- Log file created (e.g., `logs/app.log`).\n- Errors are logged with stack traces.\n- API returns useful error messages instead of crashing.","logging;error-handling;medium","Milestone 2"
"Create AI Processing Module Skeleton","Goal: Set up the Python package for AI & image processing.\n\nTasks:\n- Create `ai_processing/` package with `__init__.py`.\n- Add modules: `intent_parser.py`, `image_processor.py`, `instruction_gen.py`, `llm_integration.py`, `config.py`.\n- Define an `Intent` enum in `config.py`.\n- Ensure modules import correctly from the web server.\n\nAcceptance Criteria:\n- `from ai_processing import ...` works.\n- Basic `Intent` values are defined for movement, camera, and media.","ai;architecture;setup;critical","Milestone 3"
"Implement Rule-Based Intent Parser","Goal: Map simple natural language commands to structured intents.\n\nTasks:\n- Define keyword lists for each intent (move, center, get closer, etc.).\n- Implement `parse_intent(text) -> (intent, params)`.\n- Extract simple parameters like color (e.g., \"red\" in \"red ball\").\n- Add tests for common phrasing variants.\n\nAcceptance Criteria:\n- Sample commands map to expected intents.\n- Unknown phrases return an UNKNOWN intent.\n- Performance is fast enough for interactive use.","ai;intent-parsing;critical","Milestone 3"
"Implement Image Processing with OpenCV","Goal: Analyze camera images to detect objects and quality.\n\nTasks:\n- Implement `detect_objects(image)` using HSV color masks.\n- Implement `analyze_brightness(image)`.\n- Implement `calculate_focus_metric(image)` using Laplacian variance.\n- Implement `find_object_center(image, color)`.\n- Optionally implement simple distance estimation from size.\n\nAcceptance Criteria:\n- Colored test objects are detected in sample frames.\n- Brightness and focus metrics behave as expected.\n- Processing time per frame is reasonable (tens of ms).","ai;opencv;image-processing;critical","Milestone 3"
"Implement Instruction Generator (Intent → Commands)","Goal: Convert intents plus image analysis into concrete drone commands.\n\nTasks:\n- Define a simple `Command` representation (e.g., type + value).\n- Implement handler functions for each intent.\n- Map handlers to velocity or camera operations.\n- Use image data to adjust yaw/position for centering and distance.\n\nAcceptance Criteria:\n- For a given image + intent, commands are sensible.\n- Centering intent rotates until object is near image center.\n- Get-closer intent moves the drone forward toward the target.","ai;instruction-generation;high","Milestone 3"
"Create Chat Endpoint & AI Pipeline","Goal: Wire user chat to intent parsing, image analysis, and command execution.\n\nTasks:\n- Add `POST /chat` endpoint.\n- Parse the message into an intent and parameters.\n- Fetch latest camera image and run image processing.\n- Generate and execute commands via the ROS bridge.\n- Return a JSON response with explanation and optional image preview.\n\nAcceptance Criteria:\n- Simple messages like \"move forward\" work via chat.\n- \"Center the red ball\" triggers image-based behavior.\n- Endpoint handles errors gracefully (unknown intent, no object).","fastapi;chat;ai;high","Milestone 3"
"End-to-End Test: AI Chat Control","Goal: Validate the full chat → AI → command pipeline.\n\nTasks:\n- Test movement-only commands via chat.\n- Test centering and getting-closer commands when objects are visible.\n- Try invalid or ambiguous commands.\n- Measure response times.\n\nAcceptance Criteria:\n- Most common phrasing works as intended.\n- Latency is acceptable for interactive use.\n- No crashes on malformed inputs.","testing;ai;integration;high","Milestone 3"
"Implement Photo Capture Endpoint","Goal: Allow single-frame photo capture from the camera.\n\nTasks:\n- Add `POST /camera/capture` to save the latest image to disk.\n- Store files under a `photos/` directory with timestamped names.\n- Include drone state and basic image metrics in the response.\n\nAcceptance Criteria:\n- Endpoint saves valid image files.\n- Response contains filename, timestamp, and metadata.\n- Works whether called from UI or chat.","camera;photo;fastapi;high","Milestone 4"
"Implement Burst Photo Mode","Goal: Capture a sequence of photos at fixed intervals.\n\nTasks:\n- Add `POST /camera/burst` accepting count and interval.\n- Loop to capture multiple frames over time.\n- Return list of all photo filenames and basic info.\n\nAcceptance Criteria:\n- Correct number of photos are captured.\n- Intervals are approximately respected.\n- No major performance issues for moderate counts.","camera;photo;burst;medium","Milestone 4"
"Implement Video Recording Start/Stop","Goal: Record video from the camera stream to disk.\n\nTasks:\n- Add `POST /camera/record/start` and `/camera/record/stop`.\n- Use OpenCV VideoWriter to write frames while recording flag is set.\n- Save files under `videos/` with timestamped names.\n\nAcceptance Criteria:\n- Recorded videos play back correctly.\n- Duration roughly matches recording time.\n- No major frame drops in short tests.","camera;video;streaming;high","Milestone 4"
"Define Trajectory Format & Examples","Goal: Describe reusable drone trajectories as data.\n\nTasks:\n- Design a JSON structure for waypoints and actions.\n- Create a small set of example trajectories (circle, line, etc.).\n- Store them under a `trajectories/` folder.\n\nAcceptance Criteria:\n- Trajectory files parse correctly.\n- Structure is flexible enough for future expansion.\n- Examples cover basic use cases.","trajectory;planning;medium","Milestone 4"
"Implement Trajectory Executor","Goal: Execute predefined trajectories by following waypoints.\n\nTasks:\n- Implement a function to load a trajectory by name.\n- For each waypoint, move the drone toward the target position.\n- Trigger actions like photo capture at specified points.\n- Optionally log the flight path.\n\nAcceptance Criteria:\n- Drone follows a simple test trajectory in Gazebo.\n- Photos/videos are taken at the correct waypoints.\n- Execution handles basic errors (e.g., cannot reach waypoint).","trajectory;autonomous;planning;high","Milestone 4"
"Integrate Photo & Video into Chat","Goal: Allow chat commands to trigger media capture.\n\nTasks:\n- Add intents for TAKE_PHOTO, START_RECORDING, STOP_RECORDING.\n- Connect these intents to the camera endpoints.\n- Update chat responses to mention created files.\n\nAcceptance Criteria:\n- \"Take a photo\" via chat captures and returns a file reference.\n- \"Start recording\" and \"stop recording\" work as expected.\n- Errors (e.g., stopping when not recording) are handled cleanly.","chat;photo;video;medium","Milestone 4"
"Test All Media Features","Goal: Verify reliability of photo, burst, video, and trajectories.\n\nTasks:\n- Manually test single photo and burst modes.\n- Record several short videos and play them back.\n- Run a full trajectory while recording.\n- Look for performance or stability issues.\n\nAcceptance Criteria:\n- Media files are valid and correctly saved.\n- No crashes under normal usage patterns.\n- Behavior matches user expectations.","testing;media;integration;high","Milestone 4"
"Add Unit Tests for Core Modules","Goal: Improve confidence via automated tests.\n\nTasks:\n- Add tests for intent parser, image processor, and instruction generator.\n- Mock ROS bridge for route tests.\n- Use pytest and measure coverage.\n\nAcceptance Criteria:\n- Unit test suite runs successfully.\n- Reasonable coverage on core logic (e.g., ≥ 70–80%).","testing;unit-tests;high","Milestone 4"
"Performance Profiling & Optimization","Goal: Ensure system is performant for interactive use.\n\nTasks:\n- Profile image processing functions.\n- Profile chat endpoint end-to-end.\n- Optimize hotspots (e.g., downscale images, reuse buffers).\n\nAcceptance Criteria:\n- Image processing stays within budgeted time.\n- Chat responses remain responsive.\n- No obvious memory leaks or runaway CPU use.","optimization;performance;medium","Milestone 4"
"Write Documentation (README, Guides)","Goal: Document setup, usage, and architecture.\n\nTasks:\n- Write a clear README with features and quickstart.\n- Add setup guide for ROS 2, Gazebo, and Python env.\n- Add basic user guide for manual and chat modes.\n\nAcceptance Criteria:\n- New users can set up and run the project from docs.\n- Basic workflows are explained with examples.\n- Key architectural decisions are described.","documentation;high","Milestone 4"
"End-to-End System Validation","Goal: Run realistic scenarios to validate the whole system.\n\nTasks:\n- Run a full manual control session from the UI.\n- Run a full chat-driven session.\n- Execute at least one trajectory with media capture.\n- Observe logs and resource usage.\n\nAcceptance Criteria:\n- No critical errors across major flows.\n- System remains stable for extended sessions.\n- Behavior matches project goals.","testing;integration;system;high","Milestone 4"
"Final Bug Fixing & Polish","Goal: Clean up remaining issues and smooth the experience.\n\nTasks:\n- Fix bugs discovered during testing.\n- Improve error messages and logging.\n- Polish the UI visuals and small UX details.\n\nAcceptance Criteria:\n- No known critical or high-severity bugs remain.\n- UI feels reasonably polished for a hobby project.\n- Logs are readable and useful for debugging.","bug-fixes;polish;medium","Milestone 4"
"Project Retrospective & Future Roadmap","Goal: Reflect on the project and outline possible next steps.\n\nTasks:\n- Write a short retrospective on what worked and what didn’t.\n- List technical debt and potential improvements.\n- Sketch a roadmap (e.g., real drone, better detection, voice control).\n\nAcceptance Criteria:\n- Retrospective and roadmap are committed to the repo.\n- Future work items are captured as GitHub issues or notes.","documentation;planning;low","Milestone 4"
